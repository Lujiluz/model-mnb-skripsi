{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8dfdc4da-8b60-486d-8565-793e7876e99c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All libraries imported successfully!\n"
     ]
    }
   ],
   "source": [
    "# import libraries yang dibutuhkan\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import json\n",
    "import re\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# library buat text preprocessing\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
    "from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory\n",
    "\n",
    "# library buat machine learning\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.pipeline imporSt Pipeline\n",
    "\n",
    "# download data dari NLTK buat teknisasi NLP\n",
    "try:\n",
    "    nltk.download('punkt', quiet=True)\n",
    "    nltk.download('stopwords', quiet=True)\n",
    "    nltk.download('vader_lexicon', quiet=True)\n",
    "except:\n",
    "    pass\n",
    "\n",
    "print(\"All libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "56546c8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 352 reviews from MSH Niacinamide Brightening Moisture Gel\n",
      "Loaded 669 reviews from Skintific 5X Ceramide Barrier Repair Moisture Gel Moisturizer\n",
      "\n",
      "Total reviews loaded: 1021\n",
      "DataFrame shape: (1021, 4)\n",
      "Columns: ['review', 'page', 'scraped_at', 'product']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>page</th>\n",
       "      <th>scraped_at</th>\n",
       "      <th>product</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>awalnya sih bagus bagus aja, tapi entah kenapa...</td>\n",
       "      <td>1</td>\n",
       "      <td>2025-06-29T13:59:29.047105</td>\n",
       "      <td>MSH Niacinamide Brightening Moisture Gel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Dibandingkan dengan moisturizer yang warna bir...</td>\n",
       "      <td>1</td>\n",
       "      <td>2025-06-29T13:59:29.047139</td>\n",
       "      <td>MSH Niacinamide Brightening Moisture Gel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Sampai sekarang belum ngerasaan kalo produk in...</td>\n",
       "      <td>1</td>\n",
       "      <td>2025-06-29T13:59:29.047144</td>\n",
       "      <td>MSH Niacinamide Brightening Moisture Gel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Moisturizer paling bikin cerah di muka, memper...</td>\n",
       "      <td>1</td>\n",
       "      <td>2025-06-29T13:59:29.047172</td>\n",
       "      <td>MSH Niacinamide Brightening Moisture Gel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>aku suka banget sama moisturizer Skintific yan...</td>\n",
       "      <td>1</td>\n",
       "      <td>2025-06-29T13:59:29.047184</td>\n",
       "      <td>MSH Niacinamide Brightening Moisture Gel</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  page  \\\n",
       "0  awalnya sih bagus bagus aja, tapi entah kenapa...     1   \n",
       "1  Dibandingkan dengan moisturizer yang warna bir...     1   \n",
       "2  Sampai sekarang belum ngerasaan kalo produk in...     1   \n",
       "3  Moisturizer paling bikin cerah di muka, memper...     1   \n",
       "4  aku suka banget sama moisturizer Skintific yan...     1   \n",
       "\n",
       "                   scraped_at                                   product  \n",
       "0  2025-06-29T13:59:29.047105  MSH Niacinamide Brightening Moisture Gel  \n",
       "1  2025-06-29T13:59:29.047139  MSH Niacinamide Brightening Moisture Gel  \n",
       "2  2025-06-29T13:59:29.047144  MSH Niacinamide Brightening Moisture Gel  \n",
       "3  2025-06-29T13:59:29.047172  MSH Niacinamide Brightening Moisture Gel  \n",
       "4  2025-06-29T13:59:29.047184  MSH Niacinamide Brightening Moisture Gel  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# function buat load data review\n",
    "def load_review_data():\n",
    "    \"\"\"Load review data from available files\"\"\"\n",
    "    reviews_data = []\n",
    "    \n",
    "    review_files = [\n",
    "        'reviews_MSH Niacinamide Brightening Moisture Gel_20250629_140112.json',\n",
    "        'reviews_Skintific 5X Ceramide Barrier Repair Moisture Gel Moisturizer_20250629_004723.json'\n",
    "    ]\n",
    "    \n",
    "    for file in review_files:\n",
    "        try:\n",
    "            with open(file, 'r', encoding='utf-8') as f:\n",
    "                data = json.load(f)  # data is already a list of reviews\n",
    "                product_name = file.split('_')[1:-2]\n",
    "                product_name = ' '.join(product_name)\n",
    "                \n",
    "                # Add product info to each review\n",
    "                for review in data:  # data is the list of reviews directly\n",
    "                    review['product'] = product_name\n",
    "                    reviews_data.append(review)\n",
    "                    \n",
    "            print(f\"Loaded {len(data)} reviews from {product_name}\")\n",
    "        except FileNotFoundError:\n",
    "            print(f\"File {file} not found\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading {file}: {e}\")\n",
    "    \n",
    "    return reviews_data\n",
    "\n",
    "# Load data\n",
    "reviews_data = load_review_data()\n",
    "print(f\"\\nTotal reviews loaded: {len(reviews_data)}\")\n",
    "\n",
    "# Convert to DataFrame\n",
    "df = pd.DataFrame(reviews_data)\n",
    "print(f\"DataFrame shape: {df.shape}\")\n",
    "print(f\"Columns: {df.columns.tolist()}\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "913245a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text:\n",
      "awalnya sih bagus bagus aja, tapi entah kenapa lama kelamaan finishnya itu kayak keliatan berminyak gitu padahal pakainya dikit, terus kalau di pakai pagi hari jadi keliatan kusam banget, mencerahkannya juga perubahannya dikit banget, repurchase? no sorry\n",
      "\n",
      "Processed text:\n",
      "awal bagus bagus entah lama lama finishnya kayak liat minyak gitu padahal pakai dikit terus kalau pakai pagi hari jadi liat kusam cerah ubah dikit repurchase sorry\n",
      "awal bagus bagus entah lama lama finishnya kayak liat minyak gitu padahal pakai dikit terus kalau pakai pagi hari jadi liat kusam cerah ubah dikit repurchase sorry\n"
     ]
    }
   ],
   "source": [
    "# Text Preprocessing Class\n",
    "class IndonesianTextPreprocessor:\n",
    "    def __init__(self):\n",
    "        # Inisiasi stemmer dan stopword remover untuk Bahasa Indonesia\n",
    "        factory = StemmerFactory()\n",
    "        self.stemmer = factory.create_stemmer()\n",
    "        \n",
    "        stop_factory = StopWordRemoverFactory()\n",
    "        self.stopword_remover = stop_factory.create_stop_word_remover()\n",
    "        \n",
    "        # Additional Indonesian stopwords\n",
    "        self.additional_stopwords = {\n",
    "            'yang', 'ini', 'itu', 'dengan', 'untuk', 'dari', 'ke', 'di', 'pada', 'oleh',\n",
    "            'saya', 'aku', 'kamu', 'dia', 'mereka', 'kita', 'kalian',\n",
    "            'sangat', 'sekali', 'banget', 'bgt', 'sih', 'deh', 'dong', 'loh', 'kok',\n",
    "            'produk', 'skincare', 'wajah', 'kulit', 'moisturizer', 'gel'\n",
    "        }\n",
    "    \n",
    "    def clean_text(self, text):\n",
    "        \"\"\"Clean and normalize text\"\"\"\n",
    "        if pd.isna(text) or text == '':\n",
    "            return ''\n",
    "        \n",
    "        # Convert kata ke lowercase\n",
    "        text = str(text).lower()\n",
    "        \n",
    "        # Hapus URL (jika ada)\n",
    "        text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
    "        \n",
    "        # Hapus email address (jika ada)\n",
    "        text = re.sub(r'\\S+@\\S+', '', text)\n",
    "        \n",
    "        # Hapus angka dan karakter khusus (kaya emoticon atau puctuations), sisakan kata dan spasi\n",
    "        text = re.sub(r'[^a-zA-Z\\s]', ' ', text)\n",
    "        \n",
    "        # hapus whitespace yang berlebih\n",
    "        text = re.sub(r'\\s+', ' ', text).strip()\n",
    "        \n",
    "        return text\n",
    "    \n",
    "    def tokenize_text(self, text):\n",
    "        \"\"\"Tokenize text into words\"\"\"\n",
    "        if not text:\n",
    "            return []\n",
    "        \n",
    "        # Tokenisasi sederhana (dipisahin dari spasi)\n",
    "        tokens = text.split()\n",
    "        \n",
    "        # filter token yang kependekan\n",
    "        tokens = [token for token in tokens if len(token) >= 4 and len(token) <= 25]\n",
    "\n",
    "        return tokens\n",
    "    \n",
    "    def remove_stopwords(self, tokens):\n",
    "        \"\"\"Remove stopwords from tokens\"\"\"\n",
    "        # Join tokens to use Sastrawi stopword remover\n",
    "        text = ' '.join(tokens)\n",
    "        text = self.stopword_remover.remove(text)\n",
    "        \n",
    "        # Split back to tokens and remove additional stopwords\n",
    "        tokens = text.split()\n",
    "        tokens = [token for token in tokens if token not in self.additional_stopwords]\n",
    "        \n",
    "        return tokens\n",
    "    \n",
    "    def stem_tokens(self, tokens):\n",
    "        \"\"\"Apply stemming to tokens\"\"\"\n",
    "        # Join tokens for stemming\n",
    "        text = ' '.join(tokens)\n",
    "        stemmed_text = self.stemmer.stem(text)\n",
    "        \n",
    "        return stemmed_text.split()\n",
    "    \n",
    "    def preprocess(self, text, include_stemming=True):\n",
    "        \"\"\"Complete preprocessing pipeline\"\"\"\n",
    "        # bersihkan teks\n",
    "        cleaned_text = self.clean_text(text)\n",
    "        \n",
    "        # tokenisasi\n",
    "        tokens = self.tokenize_text(cleaned_text)\n",
    "        \n",
    "        # hapus stopwords\n",
    "        tokens = self.remove_stopwords(tokens)\n",
    "        \n",
    "        # lakukan stemming - optional (default selalu true)\n",
    "        if include_stemming:\n",
    "            tokens = self.stem_tokens(tokens)\n",
    "        \n",
    "        # kembalikan ke string\n",
    "        processed_text = ' '.join(tokens)\n",
    "        \n",
    "        return processed_text\n",
    "\n",
    "# Inisiasi preprocessor\n",
    "preprocessor = IndonesianTextPreprocessor()\n",
    "\n",
    "# Lakukan preprocessing pada contoh review\n",
    "if len(df) > 0:\n",
    "    sample_text = df['review'].iloc[0] if 'review' in df.columns else str(df.iloc[0, 0])\n",
    "    print(\"Original text:\")\n",
    "    print(sample_text)\n",
    "    print(\"\\nProcessed text:\")\n",
    "    print(preprocessor.preprocess(sample_text))\n",
    "else:\n",
    "    print(\"No data available for testing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "76b16c4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using column 'review' for text analysis\n",
      "Applying text preprocessing...\n",
      "Applying sentiment labeling...\n",
      "\n",
      "Dataset shape after processing: (1021, 6)\n",
      "\n",
      "Sentiment distribution:\n",
      "sentiment\n",
      "positive    658\n",
      "neutral     325\n",
      "negative     38\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Sample processed data:\n",
      "\n",
      "Product: MSH Niacinamide Brightening Moisture Gel\n",
      "Original: awalnya sih bagus bagus aja, tapi entah kenapa lama kelamaan finishnya itu kayak keliatan berminyak ...\n",
      "Processed: awal bagus bagus entah lama lama finishnya kayak liat minyak gitu padahal pakai dikit terus kalau pa...\n",
      "Sentiment: positive\n",
      "--------------------------------------------------\n",
      "\n",
      "Product: MSH Niacinamide Brightening Moisture Gel\n",
      "Original: Dibandingkan dengan moisturizer yang warna biru, yang ini ga terlalu moist di muka super kering aku...\n",
      "Processed: banding warna biru terlalu moist muka super kering...\n",
      "Sentiment: neutral\n",
      "--------------------------------------------------\n",
      "\n",
      "Product: MSH Niacinamide Brightening Moisture Gel\n",
      "Original: Sampai sekarang belum ngerasaan kalo produk ini bikin muka cerah. Mungkin perlu waktu yang lumayan l...\n",
      "Processed: sekarang ngerasaan kalo bikin muka cerah mungkin perlu waktu lumayan lama liat hasil mungkin butuh l...\n",
      "Sentiment: neutral\n",
      "--------------------------------------------------\n",
      "\n",
      "Product: MSH Niacinamide Brightening Moisture Gel\n",
      "Original: Moisturizer paling bikin cerah di muka, memperbaiki Skin barrier, melembabkan wajah, bisa mencerahka...\n",
      "Processed: paling bikin cerah muka baik skin barrier melembabkan cerah pake bekas jerawat bekas jerawat lebih p...\n",
      "Sentiment: positive\n",
      "--------------------------------------------------\n",
      "\n",
      "Product: MSH Niacinamide Brightening Moisture Gel\n",
      "Original: aku suka banget sama moisturizer Skintific yang bikin glowing! Ini tuh bukan yang lebay ya glowing-n...\n",
      "Processed: suka sama skintific bikin glowing bukan lebay glowing kayak sehat kenyal cahaya gitu sumpah bikin ha...\n",
      "Sentiment: positive\n",
      "--------------------------------------------------\n",
      "Applying sentiment labeling...\n",
      "\n",
      "Dataset shape after processing: (1021, 6)\n",
      "\n",
      "Sentiment distribution:\n",
      "sentiment\n",
      "positive    658\n",
      "neutral     325\n",
      "negative     38\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Sample processed data:\n",
      "\n",
      "Product: MSH Niacinamide Brightening Moisture Gel\n",
      "Original: awalnya sih bagus bagus aja, tapi entah kenapa lama kelamaan finishnya itu kayak keliatan berminyak ...\n",
      "Processed: awal bagus bagus entah lama lama finishnya kayak liat minyak gitu padahal pakai dikit terus kalau pa...\n",
      "Sentiment: positive\n",
      "--------------------------------------------------\n",
      "\n",
      "Product: MSH Niacinamide Brightening Moisture Gel\n",
      "Original: Dibandingkan dengan moisturizer yang warna biru, yang ini ga terlalu moist di muka super kering aku...\n",
      "Processed: banding warna biru terlalu moist muka super kering...\n",
      "Sentiment: neutral\n",
      "--------------------------------------------------\n",
      "\n",
      "Product: MSH Niacinamide Brightening Moisture Gel\n",
      "Original: Sampai sekarang belum ngerasaan kalo produk ini bikin muka cerah. Mungkin perlu waktu yang lumayan l...\n",
      "Processed: sekarang ngerasaan kalo bikin muka cerah mungkin perlu waktu lumayan lama liat hasil mungkin butuh l...\n",
      "Sentiment: neutral\n",
      "--------------------------------------------------\n",
      "\n",
      "Product: MSH Niacinamide Brightening Moisture Gel\n",
      "Original: Moisturizer paling bikin cerah di muka, memperbaiki Skin barrier, melembabkan wajah, bisa mencerahka...\n",
      "Processed: paling bikin cerah muka baik skin barrier melembabkan cerah pake bekas jerawat bekas jerawat lebih p...\n",
      "Sentiment: positive\n",
      "--------------------------------------------------\n",
      "\n",
      "Product: MSH Niacinamide Brightening Moisture Gel\n",
      "Original: aku suka banget sama moisturizer Skintific yang bikin glowing! Ini tuh bukan yang lebay ya glowing-n...\n",
      "Processed: suka sama skintific bikin glowing bukan lebay glowing kayak sehat kenyal cahaya gitu sumpah bikin ha...\n",
      "Sentiment: positive\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Sentiment Labeling Class\n",
    "class IndonesianSentimentLabeler:\n",
    "    def __init__(self):\n",
    "        # Indonesian positive words\n",
    "        self.positive_words = {\n",
    "            'bagus', 'baik', 'suka', 'senang', 'puas', 'mantap', 'keren', 'oke', 'cocok', \n",
    "            'recommended', 'recommend', 'love', 'amazing', 'perfect', 'excellent', 'good',\n",
    "            'lembut', 'halus', 'wangi', 'fresh', 'nyaman', 'mudah', 'cepat', 'efektif',\n",
    "            'moisturizing', 'hydrating', 'melembabkan', 'mencerahkan', 'melembutkan',\n",
    "            'terjangkau', 'murah', 'worth', 'worthed', 'satisfied', 'puas', 'puass'\n",
    "        }\n",
    "        \n",
    "        # Indonesian negative words\n",
    "        self.negative_words = {\n",
    "            'jelek', 'buruk', 'tidak', 'gak', 'ga', 'nggak', 'benci', 'kecewa', 'disappointed',\n",
    "            'bad', 'terrible', 'awful', 'worst', 'hate', 'problem', 'masalah', 'error',\n",
    "            'breakout', 'iritasi', 'gatal', 'perih', 'panas', 'berminyak', 'lengket',\n",
    "            'berat', 'sulit', 'susah', 'mahal', 'expensive', 'overpriced', 'zonk',\n",
    "            'mengecewakan', 'gagal', 'fail', 'worst'\n",
    "        }\n",
    "        \n",
    "        # Negation words that can flip sentiment\n",
    "        self.negation_words = {'tidak', 'nggak', 'gak', 'ga', 'bukan', 'never', 'no'}\n",
    "    \n",
    "    def count_sentiment_words(self, text):\n",
    "        \"\"\"Count positive and negative words in text\"\"\"\n",
    "        if not text:\n",
    "            return 0, 0\n",
    "        \n",
    "        words = text.lower().split()\n",
    "        positive_count = 0\n",
    "        negative_count = 0\n",
    "        \n",
    "        # Check for negations\n",
    "        negated = False\n",
    "        for i, word in enumerate(words):\n",
    "            if word in self.negation_words:\n",
    "                negated = True\n",
    "                continue\n",
    "            \n",
    "            if word in self.positive_words:\n",
    "                if negated:\n",
    "                    negative_count += 1\n",
    "                else:\n",
    "                    positive_count += 1\n",
    "                negated = False\n",
    "            elif word in self.negative_words:\n",
    "                if negated:\n",
    "                    positive_count += 1\n",
    "                else:\n",
    "                    negative_count += 1\n",
    "                negated = False\n",
    "            else:\n",
    "                # Reset negation if we encounter a non-sentiment word\n",
    "                if negated and i > 0 and words[i-1] in self.negation_words:\n",
    "                    continue\n",
    "                negated = False\n",
    "        \n",
    "        return positive_count, negative_count\n",
    "    \n",
    "    def label_sentiment(self, text):\n",
    "        \"\"\"Label sentiment as positive, negative, or neutral\"\"\"\n",
    "        pos_count, neg_count = self.count_sentiment_words(text)\n",
    "        \n",
    "        if pos_count > neg_count:\n",
    "            return 'positive'\n",
    "        elif neg_count > pos_count:\n",
    "            return 'negative'\n",
    "        else:\n",
    "            # For neutral cases, we can use additional heuristics\n",
    "            if 'ok' in text.lower() or 'oke' in text.lower():\n",
    "                return 'neutral'\n",
    "            elif len(text.split()) < 3:  # Very short reviews are often neutral\n",
    "                return 'neutral'\n",
    "            else:\n",
    "                return 'neutral'\n",
    "\n",
    "# Initialize sentiment labeler\n",
    "sentiment_labeler = IndonesianSentimentLabeler()\n",
    "\n",
    "# Apply preprocessing and sentiment labeling to the dataset\n",
    "if len(df) > 0:\n",
    "    # Determine the column with review text\n",
    "    text_column = None\n",
    "    for col in df.columns:\n",
    "        if 'review' in col.lower() or 'text' in col.lower() or 'comment' in col.lower():\n",
    "            text_column = col\n",
    "            break\n",
    "    \n",
    "    if text_column is None:\n",
    "        # Use the first string column\n",
    "        for col in df.columns:\n",
    "            if df[col].dtype == 'object':\n",
    "                text_column = col\n",
    "                break\n",
    "    \n",
    "    if text_column:\n",
    "        print(f\"Using column '{text_column}' for text analysis\")\n",
    "        \n",
    "        # Apply preprocessing\n",
    "        print(\"Applying text preprocessing...\")\n",
    "        df['processed_text'] = df[text_column].apply(lambda x: preprocessor.preprocess(str(x)))\n",
    "        \n",
    "        # Apply sentiment labeling\n",
    "        print(\"Applying sentiment labeling...\")\n",
    "        df['sentiment'] = df[text_column].apply(lambda x: sentiment_labeler.label_sentiment(str(x)))\n",
    "        \n",
    "        # Display results\n",
    "        print(f\"\\nDataset shape after processing: {df.shape}\")\n",
    "        print(f\"\\nSentiment distribution:\")\n",
    "        print(df['sentiment'].value_counts())\n",
    "        \n",
    "        # Show sample processed data\n",
    "        print(f\"\\nSample processed data:\")\n",
    "        sample_df = df[['product', text_column, 'processed_text', 'sentiment']].head()\n",
    "        for idx, row in sample_df.iterrows():\n",
    "            print(f\"\\nProduct: {row['product']}\")\n",
    "            print(f\"Original: {row[text_column][:100]}...\")\n",
    "            print(f\"Processed: {row['processed_text'][:100]}...\")\n",
    "            print(f\"Sentiment: {row['sentiment']}\")\n",
    "            print(\"-\" * 50)\n",
    "    else:\n",
    "        print(\"No suitable text column found in the dataset\")\n",
    "else:\n",
    "    print(\"No data available for processing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7e9a278",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Exploration and Visualization\n",
    "def explore_sentiment_data(df):\n",
    "    \"\"\"Explore and visualize sentiment data\"\"\"\n",
    "    if len(df) == 0:\n",
    "        print(\"No data to explore\")\n",
    "        return\n",
    "    \n",
    "    plt.figure(figsize=(15, 10))\n",
    "    \n",
    "    # 1. Sentiment distribution\n",
    "    plt.subplot(2, 3, 1)\n",
    "    sentiment_counts = df['sentiment'].value_counts()\n",
    "    plt.pie(sentiment_counts.values, labels=sentiment_counts.index, autopct='%1.1f%%')\n",
    "    plt.title('Sentiment Distribution')\n",
    "    \n",
    "    # 2. Sentiment by product\n",
    "    plt.subplot(2, 3, 2)\n",
    "    if 'product' in df.columns:\n",
    "        sentiment_product = pd.crosstab(df['product'], df['sentiment'])\n",
    "        sentiment_product.plot(kind='bar', stacked=True, ax=plt.gca())\n",
    "        plt.title('Sentiment by Product')\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.legend(title='Sentiment')\n",
    "    \n",
    "    # 3. Text length distribution\n",
    "    plt.subplot(2, 3, 3)\n",
    "    text_col = None\n",
    "    for col in df.columns:\n",
    "        if 'review' in col.lower() or 'text' in col.lower() or 'comment' in col.lower():\n",
    "            text_col = col\n",
    "            break\n",
    "    \n",
    "    if text_col:\n",
    "        df['text_length'] = df[text_col].astype(str).str.len()\n",
    "        plt.hist(df['text_length'], bins=30, alpha=0.7)\n",
    "        plt.title('Text Length Distribution')\n",
    "        plt.xlabel('Text Length (characters)')\n",
    "        plt.ylabel('Frequency')\n",
    "    \n",
    "    # 4. Processed text length distribution\n",
    "    plt.subplot(2, 3, 4)\n",
    "    if 'processed_text' in df.columns:\n",
    "        df['processed_length'] = df['processed_text'].str.len()\n",
    "        plt.hist(df['processed_length'], bins=30, alpha=0.7, color='orange')\n",
    "        plt.title('Processed Text Length Distribution')\n",
    "        plt.xlabel('Processed Text Length (characters)')\n",
    "        plt.ylabel('Frequency')\n",
    "    \n",
    "    # 5. Word count distribution by sentiment\n",
    "    plt.subplot(2, 3, 5)\n",
    "    if 'processed_text' in df.columns:\n",
    "        df['word_count'] = df['processed_text'].str.split().str.len()\n",
    "        for sentiment in df['sentiment'].unique():\n",
    "            subset = df[df['sentiment'] == sentiment]['word_count']\n",
    "            plt.hist(subset, alpha=0.6, label=sentiment, bins=20)\n",
    "        plt.title('Word Count by Sentiment')\n",
    "        plt.xlabel('Word Count')\n",
    "        plt.ylabel('Frequency')\n",
    "        plt.legend()\n",
    "    \n",
    "    # 6. Average word count by sentiment\n",
    "    plt.subplot(2, 3, 6)\n",
    "    if 'word_count' in df.columns:\n",
    "        avg_words = df.groupby('sentiment')['word_count'].mean()\n",
    "        plt.bar(avg_words.index, avg_words.values)\n",
    "        plt.title('Average Word Count by Sentiment')\n",
    "        plt.xlabel('Sentiment')\n",
    "        plt.ylabel('Average Word Count')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print statistics\n",
    "    print(\"\\n=== DATASET STATISTICS ===\")\n",
    "    print(f\"Total reviews: {len(df)}\")\n",
    "    print(f\"Unique products: {df['product'].nunique() if 'product' in df.columns else 'N/A'}\")\n",
    "    \n",
    "    if text_col:\n",
    "        print(f\"\\nText length statistics:\")\n",
    "        print(df['text_length'].describe())\n",
    "    \n",
    "    if 'processed_text' in df.columns:\n",
    "        print(f\"\\nProcessed text length statistics:\")\n",
    "        print(df['processed_length'].describe())\n",
    "        \n",
    "        print(f\"\\nWord count statistics:\")\n",
    "        print(df['word_count'].describe())\n",
    "    \n",
    "    print(f\"\\nSentiment distribution:\")\n",
    "    for sentiment, count in df['sentiment'].value_counts().items():\n",
    "        percentage = (count / len(df)) * 100\n",
    "        print(f\"{sentiment}: {count} ({percentage:.1f}%)\")\n",
    "\n",
    "# Explore the data\n",
    "explore_sentiment_data(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "778fdbe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MultinomialNB Sentiment Analysis Model\n",
    "class SentimentAnalysisModel:\n",
    "    def __init__(self, vectorizer_type='tfidf', max_features=5000, ngram_range=(1, 2)):\n",
    "        \"\"\"\n",
    "        Initialize the sentiment analysis model\n",
    "        \n",
    "        Parameters:\n",
    "        - vectorizer_type: 'tfidf' or 'count'\n",
    "        - max_features: maximum number of features to extract\n",
    "        - ngram_range: tuple indicating n-gram range (e.g., (1,1) for unigrams, (1,2) for unigrams+bigrams)\n",
    "        \"\"\"\n",
    "        self.vectorizer_type = vectorizer_type\n",
    "        self.max_features = max_features\n",
    "        self.ngram_range = ngram_range\n",
    "        \n",
    "        # Initialize vectorizer\n",
    "        if vectorizer_type == 'tfidf':\n",
    "            self.vectorizer = TfidfVectorizer(\n",
    "                max_features=max_features,\n",
    "                ngram_range=ngram_range,\n",
    "                stop_words=None,  # We already removed stopwords\n",
    "                lowercase=False,  # Already lowercased\n",
    "                token_pattern=r'\\b\\w+\\b'\n",
    "            )\n",
    "        else:\n",
    "            self.vectorizer = CountVectorizer(\n",
    "                max_features=max_features,\n",
    "                ngram_range=ngram_range,\n",
    "                stop_words=None,\n",
    "                lowercase=False,\n",
    "                token_pattern=r'\\b\\w+\\b'\n",
    "            )\n",
    "        \n",
    "        # Initialize MultinomialNB\n",
    "        self.model = MultinomialNB()\n",
    "        \n",
    "        # Create pipeline\n",
    "        self.pipeline = Pipeline([\n",
    "            ('vectorizer', self.vectorizer),\n",
    "            ('classifier', self.model)\n",
    "        ])\n",
    "        \n",
    "        self.is_trained = False\n",
    "    \n",
    "    def prepare_data(self, df, text_column='processed_text', target_column='sentiment'):\n",
    "        \"\"\"Prepare data for training\"\"\"\n",
    "        # Filter out empty texts\n",
    "        df_clean = df.dropna(subset=[text_column, target_column])\n",
    "        df_clean = df_clean[df_clean[text_column].str.strip() != '']\n",
    "        \n",
    "        X = df_clean[text_column].values\n",
    "        y = df_clean[target_column].values\n",
    "        \n",
    "        print(f\"Prepared {len(X)} samples for training\")\n",
    "        print(f\"Class distribution: {pd.Series(y).value_counts()}\")\n",
    "        \n",
    "        return X, y\n",
    "    \n",
    "    def train(self, X, y, test_size=0.2, random_state=42):\n",
    "        \"\"\"Train the model\"\"\"\n",
    "        # Split data\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X, y, test_size=test_size, random_state=random_state, stratify=y\n",
    "        )\n",
    "        \n",
    "        print(f\"Training set size: {len(X_train)}\")\n",
    "        print(f\"Test set size: {len(X_test)}\")\n",
    "        \n",
    "        # Train the model\n",
    "        print(\"Training MultinomialNB model...\")\n",
    "        self.pipeline.fit(X_train, y_train)\n",
    "        self.is_trained = True\n",
    "        \n",
    "        # Store test data for evaluation\n",
    "        self.X_test = X_test\n",
    "        self.y_test = y_test\n",
    "        self.X_train = X_train\n",
    "        self.y_train = y_train\n",
    "        \n",
    "        print(\"Training completed!\")\n",
    "        \n",
    "        return X_train, X_test, y_train, y_test\n",
    "    \n",
    "    def evaluate(self):\n",
    "        \"\"\"Evaluate the model\"\"\"\n",
    "        if not self.is_trained:\n",
    "            print(\"Model is not trained yet!\")\n",
    "            return\n",
    "        \n",
    "        # Predictions on training set\n",
    "        y_train_pred = self.pipeline.predict(self.X_train)\n",
    "        train_accuracy = accuracy_score(self.y_train, y_train_pred)\n",
    "        \n",
    "        # Predictions on test set\n",
    "        y_test_pred = self.pipeline.predict(self.X_test)\n",
    "        test_accuracy = accuracy_score(self.y_test, y_test_pred)\n",
    "        \n",
    "        print(\"=== MODEL EVALUATION ===\")\n",
    "        print(f\"Training Accuracy: {train_accuracy:.4f}\")\n",
    "        print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
    "        \n",
    "        print(f\"\\n=== CLASSIFICATION REPORT (Test Set) ===\")\n",
    "        print(classification_report(self.y_test, y_test_pred))\n",
    "        \n",
    "        # Confusion Matrix\n",
    "        print(f\"\\n=== CONFUSION MATRIX ===\")\n",
    "        cm = confusion_matrix(self.y_test, y_test_pred)\n",
    "        print(cm)\n",
    "        \n",
    "        # Plot confusion matrix\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "                   xticklabels=self.pipeline.classes_,\n",
    "                   yticklabels=self.pipeline.classes_)\n",
    "        plt.title('Confusion Matrix')\n",
    "        plt.xlabel('Predicted')\n",
    "        plt.ylabel('Actual')\n",
    "        plt.show()\n",
    "        \n",
    "        return {\n",
    "            'train_accuracy': train_accuracy,\n",
    "            'test_accuracy': test_accuracy,\n",
    "            'classification_report': classification_report(self.y_test, y_test_pred, output_dict=True),\n",
    "            'confusion_matrix': cm\n",
    "        }\n",
    "    \n",
    "    def cross_validate(self, X, y, cv=5):\n",
    "        \"\"\"Perform cross-validation\"\"\"\n",
    "        if not self.is_trained:\n",
    "            print(\"Training model for cross-validation...\")\n",
    "            self.pipeline.fit(X, y)\n",
    "        \n",
    "        print(f\"Performing {cv}-fold cross-validation...\")\n",
    "        cv_scores = cross_val_score(self.pipeline, X, y, cv=cv, scoring='accuracy')\n",
    "        \n",
    "        print(f\"Cross-validation scores: {cv_scores}\")\n",
    "        print(f\"Mean CV accuracy: {cv_scores.mean():.4f} (+/- {cv_scores.std() * 2:.4f})\")\n",
    "        \n",
    "        return cv_scores\n",
    "    \n",
    "    def predict(self, texts):\n",
    "        \"\"\"Make predictions on new texts\"\"\"\n",
    "        if not self.is_trained:\n",
    "            print(\"Model is not trained yet!\")\n",
    "            return None\n",
    "        \n",
    "        if isinstance(texts, str):\n",
    "            texts = [texts]\n",
    "        \n",
    "        predictions = self.pipeline.predict(texts)\n",
    "        probabilities = self.pipeline.predict_proba(texts)\n",
    "        \n",
    "        results = []\n",
    "        for i, text in enumerate(texts):\n",
    "            result = {\n",
    "                'text': text,\n",
    "                'predicted_sentiment': predictions[i],\n",
    "                'probabilities': {\n",
    "                    class_name: prob \n",
    "                    for class_name, prob in zip(self.pipeline.classes_, probabilities[i])\n",
    "                }\n",
    "            }\n",
    "            results.append(result)\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def get_feature_importance(self, top_n=20):\n",
    "        \"\"\"Get most important features for each class\"\"\"\n",
    "        if not self.is_trained:\n",
    "            print(\"Model is not trained yet!\")\n",
    "            return\n",
    "        \n",
    "        feature_names = self.vectorizer.get_feature_names_out()\n",
    "        classes = self.pipeline.classes_\n",
    "        \n",
    "        # Get feature log probabilities\n",
    "        feature_log_prob = self.model.feature_log_prob_\n",
    "        \n",
    "        print(\"=== TOP FEATURES BY CLASS ===\")\n",
    "        for i, class_name in enumerate(classes):\n",
    "            top_features_idx = feature_log_prob[i].argsort()[-top_n:][::-1]\n",
    "            top_features = [(feature_names[idx], feature_log_prob[i][idx]) for idx in top_features_idx]\n",
    "            \n",
    "            print(f\"\\nTop {top_n} features for {class_name}:\")\n",
    "            for feature, prob in top_features:\n",
    "                print(f\"  {feature}: {prob:.4f}\")\n",
    "\n",
    "# Train the model if we have data\n",
    "if len(df) > 0 and 'processed_text' in df.columns and 'sentiment' in df.columns:\n",
    "    print(\"Initializing sentiment analysis model...\")\n",
    "    model = SentimentAnalysisModel(vectorizer_type='tfidf', max_features=3000, ngram_range=(1, 2))\n",
    "    \n",
    "    # Prepare data\n",
    "    X, y = model.prepare_data(df)\n",
    "    \n",
    "    if len(X) > 0:\n",
    "        # Train the model\n",
    "        X_train, X_test, y_train, y_test = model.train(X, y)\n",
    "        \n",
    "        # Evaluate the model\n",
    "        evaluation_results = model.evaluate()\n",
    "        \n",
    "        # Cross-validation\n",
    "        cv_scores = model.cross_validate(X, y)\n",
    "        \n",
    "        # Show feature importance\n",
    "        model.get_feature_importance(top_n=15)\n",
    "    else:\n",
    "        print(\"No valid data available for training\")\n",
    "else:\n",
    "    print(\"Required columns not found. Please run previous cells first.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6821a2a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Testing and Hyperparameter Tuning\n",
    "def test_model_predictions(model, preprocessor):\n",
    "    \"\"\"Test model with sample predictions\"\"\"\n",
    "    if not model.is_trained:\n",
    "        print(\"Model is not trained yet!\")\n",
    "        return\n",
    "    \n",
    "    # Sample test texts (in Indonesian)\n",
    "    test_texts = [\n",
    "        \"Produk ini sangat bagus dan membuat kulit saya lembut sekali\",\n",
    "        \"Saya tidak suka produk ini karena membuat kulit berminyak\",\n",
    "        \"Produk biasa saja, tidak ada perubahan yang signifikan\",\n",
    "        \"Amazing product! Highly recommended for sensitive skin\",\n",
    "        \"Terrible product, caused breakout on my skin\",\n",
    "        \"Moisturizer ini cocok banget untuk kulit kering saya\",\n",
    "        \"Gak cocok di kulit saya, malah jadi bruntusan\",\n",
    "        \"Harga terjangkau dan kualitas bagus\",\n",
    "        \"Overpriced untuk kualitas yang biasa aja\",\n",
    "        \"Love this gel moisturizer, very hydrating\"\n",
    "    ]\n",
    "    \n",
    "    print(\"=== TESTING MODEL WITH SAMPLE TEXTS ===\")\n",
    "    for i, text in enumerate(test_texts, 1):\n",
    "        # Preprocess text\n",
    "        processed_text = preprocessor.preprocess(text)\n",
    "        \n",
    "        # Get prediction\n",
    "        results = model.predict([processed_text])\n",
    "        result = results[0]\n",
    "        \n",
    "        print(f\"\\n{i}. Original: {text}\")\n",
    "        print(f\"   Processed: {processed_text}\")\n",
    "        print(f\"   Predicted: {result['predicted_sentiment']}\")\n",
    "        print(f\"   Probabilities: {result['probabilities']}\")\n",
    "\n",
    "def hyperparameter_tuning(X, y):\n",
    "    \"\"\"Perform hyperparameter tuning for the model\"\"\"\n",
    "    print(\"=== HYPERPARAMETER TUNING ===\")\n",
    "    \n",
    "    # Define parameter grid\n",
    "    param_grid = {\n",
    "        'vectorizer__max_features': [1000, 3000, 5000],\n",
    "        'vectorizer__ngram_range': [(1, 1), (1, 2), (1, 3)],\n",
    "        'classifier__alpha': [0.1, 0.5, 1.0, 2.0]\n",
    "    }\n",
    "    \n",
    "    # Create pipeline for grid search\n",
    "    pipeline = Pipeline([\n",
    "        ('vectorizer', TfidfVectorizer(stop_words=None, lowercase=False)),\n",
    "        ('classifier', MultinomialNB())\n",
    "    ])\n",
    "    \n",
    "    # Grid search with cross-validation\n",
    "    grid_search = GridSearchCV(\n",
    "        pipeline, \n",
    "        param_grid, \n",
    "        cv=3,  # Use 3-fold CV for faster execution\n",
    "        scoring='accuracy',\n",
    "        n_jobs=-1,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    print(\"Starting grid search...\")\n",
    "    grid_search.fit(X, y)\n",
    "    \n",
    "    print(f\"\\nBest parameters: {grid_search.best_params_}\")\n",
    "    print(f\"Best cross-validation score: {grid_search.best_score_:.4f}\")\n",
    "    \n",
    "    # Train model with best parameters\n",
    "    best_model = grid_search.best_estimator_\n",
    "    \n",
    "    return best_model, grid_search.best_params_\n",
    "\n",
    "def analyze_misclassifications(model):\n",
    "    \"\"\"Analyze misclassified examples\"\"\"\n",
    "    if not model.is_trained:\n",
    "        print(\"Model is not trained yet!\")\n",
    "        return\n",
    "    \n",
    "    # Get predictions\n",
    "    y_pred = model.pipeline.predict(model.X_test)\n",
    "    \n",
    "    # Find misclassified examples\n",
    "    misclassified_idx = []\n",
    "    for i, (true_label, pred_label) in enumerate(zip(model.y_test, y_pred)):\n",
    "        if true_label != pred_label:\n",
    "            misclassified_idx.append(i)\n",
    "    \n",
    "    print(f\"=== MISCLASSIFICATION ANALYSIS ===\")\n",
    "    print(f\"Total misclassified: {len(misclassified_idx)} out of {len(model.y_test)}\")\n",
    "    \n",
    "    # Show some examples\n",
    "    print(f\"\\nSample misclassified examples:\")\n",
    "    for i, idx in enumerate(misclassified_idx[:10]):  # Show first 10\n",
    "        text = model.X_test[idx]\n",
    "        true_label = model.y_test[idx]\n",
    "        pred_label = y_pred[idx]\n",
    "        \n",
    "        print(f\"\\n{i+1}. Text: {text[:100]}...\")\n",
    "        print(f\"   True: {true_label}, Predicted: {pred_label}\")\n",
    "\n",
    "# Run additional tests if model is available\n",
    "if 'model' in locals() and model.is_trained:\n",
    "    # Test with sample predictions\n",
    "    test_model_predictions(model, preprocessor)\n",
    "    \n",
    "    # Analyze misclassifications\n",
    "    analyze_misclassifications(model)\n",
    "    \n",
    "    # Hyperparameter tuning (commented out to save time, uncomment if needed)\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"HYPERPARAMETER TUNING\")\n",
    "    print(\"This may take several minutes...\")\n",
    "    \n",
    "    # Uncomment the lines below to run hyperparameter tuning\n",
    "    # best_model, best_params = hyperparameter_tuning(X, y)\n",
    "    # print(f\"Best model accuracy: {best_model.score(model.X_test, model.y_test):.4f}\")\n",
    "    \n",
    "    print(\"\\nNote: Hyperparameter tuning is commented out to save time.\")\n",
    "    print(\"Uncomment the lines above to run it.\")\n",
    "else:\n",
    "    print(\"Model not available. Please run the training cell first.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eac93b2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Saving and Final Analysis\n",
    "import pickle\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "def save_model_and_results(model, evaluation_results, cv_scores, filename_prefix='sentiment_model'):\n",
    "    \"\"\"Save the trained model and results\"\"\"\n",
    "    if not model.is_trained:\n",
    "        print(\"Model is not trained yet!\")\n",
    "        return\n",
    "    \n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    \n",
    "    # Save the model\n",
    "    model_filename = f\"{filename_prefix}_{timestamp}.pkl\"\n",
    "    with open(model_filename, 'wb') as f:\n",
    "        pickle.dump(model, f)\n",
    "    print(f\"Model saved as: {model_filename}\")\n",
    "    \n",
    "    # Save evaluation results\n",
    "    results = {\n",
    "        'timestamp': timestamp,\n",
    "        'model_type': 'MultinomialNB',\n",
    "        'vectorizer_type': model.vectorizer_type,\n",
    "        'max_features': model.max_features,\n",
    "        'ngram_range': model.ngram_range,\n",
    "        'train_accuracy': evaluation_results['train_accuracy'],\n",
    "        'test_accuracy': evaluation_results['test_accuracy'],\n",
    "        'classification_report': evaluation_results['classification_report'],\n",
    "        'cv_scores': cv_scores.tolist(),\n",
    "        'cv_mean': cv_scores.mean(),\n",
    "        'cv_std': cv_scores.std()\n",
    "    }\n",
    "    \n",
    "    results_filename = f\"evaluation_results_{timestamp}.json\"\n",
    "    with open(results_filename, 'w', encoding='utf-8') as f:\n",
    "        json.dump(results, f, indent=2, ensure_ascii=False)\n",
    "    print(f\"Evaluation results saved as: {results_filename}\")\n",
    "    \n",
    "    return model_filename, results_filename\n",
    "\n",
    "def load_model(filename):\n",
    "    \"\"\"Load a saved model\"\"\"\n",
    "    try:\n",
    "        with open(filename, 'rb') as f:\n",
    "            model = pickle.load(f)\n",
    "        print(f\"Model loaded from: {filename}\")\n",
    "        return model\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Model file {filename} not found!\")\n",
    "        return None\n",
    "\n",
    "def create_sentiment_analysis_report(df, model, evaluation_results, cv_scores):\n",
    "    \"\"\"Create a comprehensive analysis report\"\"\"\n",
    "    report = []\n",
    "    report.append(\"=\"*60)\n",
    "    report.append(\"SENTIMENT ANALYSIS REPORT\")\n",
    "    report.append(\"=\"*60)\n",
    "    report.append(f\"Generated on: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "    report.append(\"\")\n",
    "    \n",
    "    # Dataset Statistics\n",
    "    report.append(\"DATASET STATISTICS:\")\n",
    "    report.append(f\"- Total reviews: {len(df)}\")\n",
    "    report.append(f\"- Unique products: {df['product'].nunique() if 'product' in df.columns else 'N/A'}\")\n",
    "    \n",
    "    if 'sentiment' in df.columns:\n",
    "        sentiment_dist = df['sentiment'].value_counts()\n",
    "        report.append(\"- Sentiment distribution:\")\n",
    "        for sentiment, count in sentiment_dist.items():\n",
    "            percentage = (count / len(df)) * 100\n",
    "            report.append(f\"  {sentiment}: {count} ({percentage:.1f}%)\")\n",
    "    \n",
    "    report.append(\"\")\n",
    "    \n",
    "    # Model Configuration\n",
    "    report.append(\"MODEL CONFIGURATION:\")\n",
    "    report.append(f\"- Algorithm: MultinomialNB\")\n",
    "    report.append(f\"- Vectorizer: {model.vectorizer_type.upper()}\")\n",
    "    report.append(f\"- Max features: {model.max_features}\")\n",
    "    report.append(f\"- N-gram range: {model.ngram_range}\")\n",
    "    report.append(\"\")\n",
    "    \n",
    "    # Model Performance\n",
    "    report.append(\"MODEL PERFORMANCE:\")\n",
    "    report.append(f\"- Training accuracy: {evaluation_results['train_accuracy']:.4f}\")\n",
    "    report.append(f\"- Test accuracy: {evaluation_results['test_accuracy']:.4f}\")\n",
    "    report.append(f\"- Cross-validation mean: {cv_scores.mean():.4f} (+/- {cv_scores.std() * 2:.4f})\")\n",
    "    report.append(\"\")\n",
    "    \n",
    "    # Classification Report\n",
    "    report.append(\"DETAILED CLASSIFICATION METRICS:\")\n",
    "    class_report = evaluation_results['classification_report']\n",
    "    for class_name in ['negative', 'neutral', 'positive']:\n",
    "        if class_name in class_report:\n",
    "            metrics = class_report[class_name]\n",
    "            report.append(f\"{class_name.upper()}:\")\n",
    "            report.append(f\"  Precision: {metrics['precision']:.4f}\")\n",
    "            report.append(f\"  Recall: {metrics['recall']:.4f}\")\n",
    "            report.append(f\"  F1-score: {metrics['f1-score']:.4f}\")\n",
    "            report.append(f\"  Support: {metrics['support']}\")\n",
    "            report.append(\"\")\n",
    "    \n",
    "    # Overall metrics\n",
    "    if 'macro avg' in class_report:\n",
    "        macro_avg = class_report['macro avg']\n",
    "        report.append(\"MACRO AVERAGE:\")\n",
    "        report.append(f\"  Precision: {macro_avg['precision']:.4f}\")\n",
    "        report.append(f\"  Recall: {macro_avg['recall']:.4f}\")\n",
    "        report.append(f\"  F1-score: {macro_avg['f1-score']:.4f}\")\n",
    "        report.append(\"\")\n",
    "    \n",
    "    # Recommendations\n",
    "    report.append(\"RECOMMENDATIONS:\")\n",
    "    test_acc = evaluation_results['test_accuracy']\n",
    "    if test_acc >= 0.8:\n",
    "        report.append(\"- Model performance is excellent (>80% accuracy)\")\n",
    "    elif test_acc >= 0.7:\n",
    "        report.append(\"- Model performance is good (70-80% accuracy)\")\n",
    "    elif test_acc >= 0.6:\n",
    "        report.append(\"- Model performance is acceptable (60-70% accuracy)\")\n",
    "        report.append(\"- Consider collecting more training data\")\n",
    "        report.append(\"- Try different preprocessing techniques\")\n",
    "    else:\n",
    "        report.append(\"- Model performance needs improvement (<60% accuracy)\")\n",
    "        report.append(\"- Collect more diverse training data\")\n",
    "        report.append(\"- Review preprocessing and feature extraction\")\n",
    "        report.append(\"- Consider different algorithms or ensemble methods\")\n",
    "    \n",
    "    report.append(\"\")\n",
    "    report.append(\"=\"*60)\n",
    "    \n",
    "    # Save report\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    report_filename = f\"sentiment_analysis_report_{timestamp}.txt\"\n",
    "    with open(report_filename, 'w', encoding='utf-8') as f:\n",
    "        f.write('\\n'.join(report))\n",
    "    \n",
    "    print(f\"Analysis report saved as: {report_filename}\")\n",
    "    \n",
    "    # Print report\n",
    "    print('\\n'.join(report))\n",
    "    \n",
    "    return report_filename\n",
    "\n",
    "# Execute final analysis if model is available\n",
    "if 'model' in locals() and 'evaluation_results' in locals() and 'cv_scores' in locals():\n",
    "    print(\"Creating final analysis and saving results...\")\n",
    "    \n",
    "    # Save model and results\n",
    "    model_file, results_file = save_model_and_results(model, evaluation_results, cv_scores)\n",
    "    \n",
    "    # Create comprehensive report\n",
    "    report_file = create_sentiment_analysis_report(df, model, evaluation_results, cv_scores)\n",
    "    \n",
    "    print(f\"\\n=== FILES CREATED ===\")\n",
    "    print(f\"1. Model file: {model_file}\")\n",
    "    print(f\"2. Results file: {results_file}\")\n",
    "    print(f\"3. Analysis report: {report_file}\")\n",
    "    \n",
    "    print(f\"\\n=== NEXT STEPS ===\")\n",
    "    print(\"1. Review the analysis report for insights\")\n",
    "    print(\"2. Use the saved model for future predictions\")\n",
    "    print(\"3. Consider collecting more data if accuracy is low\")\n",
    "    print(\"4. Experiment with different preprocessing techniques\")\n",
    "    print(\"5. Try hyperparameter tuning for better performance\")\n",
    "    \n",
    "else:\n",
    "    print(\"Please run all previous cells to train the model first.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "main_program_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
